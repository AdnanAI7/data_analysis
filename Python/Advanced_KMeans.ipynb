{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337e0555",
   "metadata": {},
   "source": [
    "# Advanced K-Means Clustering from Scratch and Comparison with scikit-learn\n",
    "\n",
    "This notebook implements a highly challenging K-Means clustering algorithm from scratch in Python, explains the code step-by-step, and compares it against scikit-learn's implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61e9de2",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans as SklearnKMeans\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "import random\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43415863",
   "metadata": {},
   "source": [
    "## 2. Generate Complex Synthetic Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588218a4",
   "metadata": {},
   "source": [
    "## 2. Generate Complex Synthetic Dataset\n",
    "Create a synthetic dataset with overlapping clusters, varying densities, and outliers to challenge the K-Means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb791f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_complex_dataset(n_samples=1000, n_features=2, n_clusters=5):\n",
    "    \"\"\"\n",
    "    Generate a complex dataset with overlapping clusters, varying densities, and outliers.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of samples in the dataset\n",
    "    n_features : int\n",
    "        Number of features (dimensions)\n",
    "    n_clusters : int\n",
    "        Number of true clusters to generate\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X : numpy.ndarray\n",
    "        Generated dataset\n",
    "    true_labels : numpy.ndarray\n",
    "        True cluster labels for each data point\n",
    "    \"\"\"\n",
    "    # Initialize arrays to store data and labels\n",
    "    X = np.zeros((n_samples, n_features))\n",
    "    true_labels = np.zeros(n_samples, dtype=int)\n",
    "    \n",
    "    # Track current position in the dataset\n",
    "    current_pos = 0\n",
    "    \n",
    "    # Create clusters with varying densities and spreads\n",
    "    for i in range(n_clusters):\n",
    "        # Determine number of samples in this cluster (varying sizes)\n",
    "        if i == n_clusters - 1:\n",
    "            cluster_samples = n_samples - current_pos\n",
    "        else:\n",
    "            # Random cluster size, weighted toward the beginning clusters\n",
    "            weight = 1.0 - 0.5 * (i / n_clusters)\n",
    "            cluster_samples = int((n_samples / n_clusters) * weight)\n",
    "            cluster_samples = min(cluster_samples, n_samples - current_pos - (n_clusters - i - 1))\n",
    "        \n",
    "        # Generate cluster center\n",
    "        center = np.random.uniform(-10, 10, size=n_features)\n",
    "        \n",
    "        # Vary the spread of each cluster\n",
    "        spread = np.random.uniform(0.5, 2.5)\n",
    "        \n",
    "        # Generate cluster data with normal distribution\n",
    "        cluster_data = np.random.normal(loc=center, scale=spread, size=(cluster_samples, n_features))\n",
    "        \n",
    "        # Add the cluster data to our dataset\n",
    "        X[current_pos:current_pos + cluster_samples] = cluster_data\n",
    "        true_labels[current_pos:current_pos + cluster_samples] = i\n",
    "        \n",
    "        current_pos += cluster_samples\n",
    "    \n",
    "    # Add some outliers (random points far from all clusters)\n",
    "    n_outliers = int(n_samples * 0.05)  # 5% outliers\n",
    "    outlier_indices = np.random.choice(n_samples, n_outliers, replace=False)\n",
    "    \n",
    "    for idx in outlier_indices:\n",
    "        # Generate random point far from the center\n",
    "        X[idx] = np.random.uniform(-20, 20, size=n_features)\n",
    "        # Label outliers as -1 (will be helpful for evaluation)\n",
    "        true_labels[idx] = -1\n",
    "    \n",
    "    # Introduce some overlap between clusters\n",
    "    n_overlap = int(n_samples * 0.1)  # 10% overlap\n",
    "    overlap_indices = np.random.choice(n_samples, n_overlap, replace=False)\n",
    "    \n",
    "    for idx in overlap_indices:\n",
    "        if true_labels[idx] != -1:  # Don't overlap outliers\n",
    "            # Move this point toward a neighboring cluster\n",
    "            target_cluster = (true_labels[idx] + 1) % n_clusters\n",
    "            # Find center of target cluster\n",
    "            target_points = X[true_labels == target_cluster]\n",
    "            if len(target_points) > 0:\n",
    "                target_center = np.mean(target_points, axis=0)\n",
    "                # Move 70% of the way from original cluster to target cluster\n",
    "                original_pos = X[idx].copy()\n",
    "                X[idx] = original_pos + 0.7 * (target_center - original_pos)\n",
    "    \n",
    "    return X, true_labels\n",
    "\n",
    "# Generate dataset\n",
    "X, true_labels = generate_complex_dataset(n_samples=1000, n_features=2, n_clusters=5)\n",
    "\n",
    "# Visualize the generated dataset\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=true_labels, cmap='viridis', s=30, alpha=0.7)\n",
    "plt.title('Generated Complex Dataset with 5 Clusters')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Number of unique clusters: {len(np.unique(true_labels))}\")\n",
    "print(f\"Cluster sizes: {np.bincount(true_labels[true_labels >= 0])}\")\n",
    "print(f\"Number of outliers: {np.sum(true_labels == -1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958dd384",
   "metadata": {},
   "source": [
    "## 3. Implement Custom K-Means Algorithm from Scratch\n",
    "\n",
    "Below we'll implement a custom K-Means algorithm from scratch, including:\n",
    "- Random centroid initialization\n",
    "- Distance calculation\n",
    "- Cluster assignment\n",
    "- Centroid updating\n",
    "- Convergence checking\n",
    "- Handling edge cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c34b05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    \"\"\"\n",
    "    Custom implementation of K-Means clustering algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_clusters : int, default=8\n",
    "        The number of clusters to form.\n",
    "    \n",
    "    max_iter : int, default=300\n",
    "        Maximum number of iterations for a single run.\n",
    "    \n",
    "    tol : float, default=1e-4\n",
    "        Relative tolerance with regards to inertia to declare convergence.\n",
    "    \n",
    "    n_init : int, default=10\n",
    "        Number of times the algorithm will be run with different centroid seeds.\n",
    "        \n",
    "    init : str, default='random'\n",
    "        Method for initialization ('random' or 'kmeans++').\n",
    "        \n",
    "    random_state : int, default=None\n",
    "        Determines random number generation for centroid initialization.\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    centroids : numpy.ndarray\n",
    "        Coordinates of cluster centers.\n",
    "    \n",
    "    labels : numpy.ndarray\n",
    "        Labels of each point.\n",
    "    \n",
    "    inertia : float\n",
    "        Sum of squared distances of samples to their closest cluster center.\n",
    "        \n",
    "    n_iter : int\n",
    "        Number of iterations run.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_clusters=8, max_iter=300, tol=1e-4, n_init=10, \n",
    "                 init='random', random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.n_init = n_init\n",
    "        self.init = init\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Attributes that will be set during fitting\n",
    "        self.centroids = None\n",
    "        self.labels = None\n",
    "        self.inertia = None\n",
    "        self.n_iter = None\n",
    "    \n",
    "    def _init_centroids(self, X):\n",
    "        \"\"\"\n",
    "        Initialize centroids either randomly or using k-means++.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray\n",
    "            Training data.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        centroids : numpy.ndarray\n",
    "            Initial positions of centroids.\n",
    "        \"\"\"\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        if self.init == 'random':\n",
    "            # Random initialization: Choose random data points as initial centroids\n",
    "            idx = np.random.choice(n_samples, self.n_clusters, replace=False)\n",
    "            centroids = X[idx].copy()\n",
    "        elif self.init == 'kmeans++':\n",
    "            # KMeans++ initialization will be implemented separately\n",
    "            centroids = self._kmeans_plus_plus_init(X)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid init method. Use 'random' or 'kmeans++'\")\n",
    "        \n",
    "        return centroids\n",
    "    \n",
    "    def _assign_clusters(self, X, centroids):\n",
    "        \"\"\"\n",
    "        Assign each data point to the nearest centroid.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray\n",
    "            Training data.\n",
    "        centroids : numpy.ndarray\n",
    "            Current centroid positions.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        labels : numpy.ndarray\n",
    "            Cluster assignments for each data point.\n",
    "        distances : numpy.ndarray\n",
    "            Distance from each point to its assigned centroid.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Initialize arrays to store distances and labels\n",
    "        distances = np.zeros((n_samples, self.n_clusters))\n",
    "        \n",
    "        # Calculate Euclidean distance from each point to each centroid\n",
    "        for k in range(self.n_clusters):\n",
    "            # Vectorized distance calculation\n",
    "            diff = X - centroids[k]\n",
    "            sq_dist = np.sum(diff**2, axis=1)\n",
    "            distances[:, k] = sq_dist\n",
    "        \n",
    "        # Assign each point to the nearest centroid\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "        \n",
    "        # Get the distances to the assigned centroids\n",
    "        min_distances = np.min(distances, axis=1)\n",
    "        \n",
    "        return labels, min_distances\n",
    "    \n",
    "    def _update_centroids(self, X, labels):\n",
    "        \"\"\"\n",
    "        Update centroid positions based on assigned clusters.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray\n",
    "            Training data.\n",
    "        labels : numpy.ndarray\n",
    "            Current cluster assignments.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        new_centroids : numpy.ndarray\n",
    "            Updated centroid positions.\n",
    "        empty_clusters : list\n",
    "            Indices of any empty clusters.\n",
    "        \"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        new_centroids = np.zeros((self.n_clusters, n_features))\n",
    "        empty_clusters = []\n",
    "        \n",
    "        for k in range(self.n_clusters):\n",
    "            # Get all points assigned to cluster k\n",
    "            cluster_points = X[labels == k]\n",
    "            \n",
    "            if len(cluster_points) == 0:\n",
    "                # Handle empty cluster\n",
    "                empty_clusters.append(k)\n",
    "                # Keep the old centroid for now\n",
    "                new_centroids[k] = self.centroids[k]\n",
    "            else:\n",
    "                # Update centroid to mean of all points in the cluster\n",
    "                new_centroids[k] = np.mean(cluster_points, axis=0)\n",
    "        \n",
    "        return new_centroids, empty_clusters\n",
    "    \n",
    "    def _handle_empty_clusters(self, X, labels, empty_clusters):\n",
    "        \"\"\"\n",
    "        Handle empty clusters by assigning a new centroid.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray\n",
    "            Training data.\n",
    "        labels : numpy.ndarray\n",
    "            Current cluster assignments.\n",
    "        empty_clusters : list\n",
    "            Indices of empty clusters.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        centroids : numpy.ndarray\n",
    "            Updated centroids with new positions for empty clusters.\n",
    "        \"\"\"\n",
    "        centroids = self.centroids.copy()\n",
    "        \n",
    "        for k in empty_clusters:\n",
    "            # Strategy: Find the point furthest from its centroid\n",
    "            # and make it the new centroid for the empty cluster\n",
    "            \n",
    "            # Get distances from each point to its assigned centroid\n",
    "            distances = np.zeros(X.shape[0])\n",
    "            for i, x in enumerate(X):\n",
    "                assigned_cluster = labels[i]\n",
    "                distances[i] = np.sum((x - centroids[assigned_cluster])**2)\n",
    "            \n",
    "            # Find the point furthest from its assigned centroid\n",
    "            furthest_point_idx = np.argmax(distances)\n",
    "            \n",
    "            # Assign this point as the new centroid\n",
    "            centroids[k] = X[furthest_point_idx]\n",
    "            \n",
    "            # Update the point's cluster assignment\n",
    "            labels[furthest_point_idx] = k\n",
    "        \n",
    "        return centroids, labels\n",
    "    \n",
    "    def _calculate_inertia(self, X, labels, centroids):\n",
    "        \"\"\"\n",
    "        Calculate the inertia (sum of squared distances to centroids).\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray\n",
    "            Training data.\n",
    "        labels : numpy.ndarray\n",
    "            Cluster assignments.\n",
    "        centroids : numpy.ndarray\n",
    "            Centroid positions.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        inertia : float\n",
    "            Sum of squared distances of samples to their closest cluster center.\n",
    "        \"\"\"\n",
    "        inertia = 0.0\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            # Distance from point to its assigned centroid\n",
    "            centroid = centroids[labels[i]]\n",
    "            inertia += np.sum((X[i] - centroid)**2)\n",
    "        \n",
    "        return inertia\n",
    "    \n",
    "    def _single_run(self, X):\n",
    "        \"\"\"\n",
    "        Run K-Means algorithm once with a given initialization.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray\n",
    "            Training data.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        centroids : numpy.ndarray\n",
    "            Final centroid positions.\n",
    "        labels : numpy.ndarray\n",
    "            Final cluster assignments.\n",
    "        inertia : float\n",
    "            Final inertia value.\n",
    "        n_iter : int\n",
    "            Number of iterations run.\n",
    "        \"\"\"\n",
    "        # Initialize centroids\n",
    "        centroids = self._init_centroids(X)\n",
    "        \n",
    "        # Initialize variables\n",
    "        previous_inertia = float('inf')\n",
    "        labels = None\n",
    "        n_iter = 0\n",
    "        \n",
    "        # Main loop\n",
    "        for iteration in range(self.max_iter):\n",
    "            # Assign clusters\n",
    "            labels, min_distances = self._assign_clusters(X, centroids)\n",
    "            \n",
    "            # Update centroids\n",
    "            new_centroids, empty_clusters = self._update_centroids(X, labels)\n",
    "            \n",
    "            # Handle empty clusters if any\n",
    "            if empty_clusters:\n",
    "                new_centroids, labels = self._handle_empty_clusters(X, labels, empty_clusters)\n",
    "            \n",
    "            # Calculate inertia\n",
    "            inertia = self._calculate_inertia(X, labels, centroids)\n",
    "            \n",
    "            # Check for convergence\n",
    "            inertia_change = previous_inertia - inertia\n",
    "            if abs(inertia_change) < self.tol * previous_inertia:\n",
    "                break\n",
    "                \n",
    "            # Update for next iteration\n",
    "            previous_inertia = inertia\n",
    "            centroids = new_centroids\n",
    "            n_iter = iteration + 1\n",
    "        \n",
    "        return centroids, labels, inertia, n_iter\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Compute K-Means clustering.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray\n",
    "            Training data.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Fitted estimator.\n",
    "        \"\"\"\n",
    "        # Run K-Means multiple times and select the best result\n",
    "        best_inertia = float('inf')\n",
    "        best_centroids = None\n",
    "        best_labels = None\n",
    "        best_n_iter = None\n",
    "        \n",
    "        for _ in range(self.n_init):\n",
    "            centroids, labels, inertia, n_iter = self._single_run(X)\n",
    "            \n",
    "            if inertia < best_inertia:\n",
    "                best_centroids = centroids\n",
    "                best_labels = labels\n",
    "                best_inertia = inertia\n",
    "                best_n_iter = n_iter\n",
    "        \n",
    "        # Store the best results\n",
    "        self.centroids = best_centroids\n",
    "        self.labels = best_labels\n",
    "        self.inertia = best_inertia\n",
    "        self.n_iter = best_n_iter\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the closest cluster for each sample in X.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray\n",
    "            New data to predict.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        labels : numpy.ndarray\n",
    "            Index of the cluster each sample belongs to.\n",
    "        \"\"\"\n",
    "        if self.centroids is None:\n",
    "            raise ValueError(\"Model has not been fitted yet.\")\n",
    "        \n",
    "        labels, _ = self._assign_clusters(X, self.centroids)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59951742",
   "metadata": {},
   "source": [
    "## 4. Initialize Centroids with Advanced Techniques\n",
    "\n",
    "One of the key challenges in K-Means clustering is initializing the centroids. Bad initialization can lead to poor results. We'll implement the K-Means++ algorithm, which selects initial centroids that are far apart from each other, leading to better clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dc15ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the K-Means++ initialization method to our KMeans class\n",
    "def _kmeans_plus_plus_init(self, X):\n",
    "    \"\"\"\n",
    "    Initialize centroids using the K-Means++ method.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Training data.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    centroids : numpy.ndarray\n",
    "        Initial positions of centroids.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    centroids = np.zeros((self.n_clusters, n_features))\n",
    "    \n",
    "    # Choose the first centroid randomly\n",
    "    if self.random_state is not None:\n",
    "        np.random.seed(self.random_state)\n",
    "        \n",
    "    first_centroid_idx = np.random.choice(n_samples)\n",
    "    centroids[0] = X[first_centroid_idx]\n",
    "    \n",
    "    # Choose the remaining centroids\n",
    "    for k in range(1, self.n_clusters):\n",
    "        # Calculate the squared distances from each point to the nearest existing centroid\n",
    "        distances = np.zeros(n_samples)\n",
    "        for i, x in enumerate(X):\n",
    "            # Find the minimum distance to any existing centroid\n",
    "            min_dist = float('inf')\n",
    "            for j in range(k):\n",
    "                dist = np.sum((x - centroids[j])**2)\n",
    "                min_dist = min(min_dist, dist)\n",
    "            distances[i] = min_dist\n",
    "        \n",
    "        # Normalize distances to create a probability distribution\n",
    "        distances_sum = np.sum(distances)\n",
    "        if distances_sum > 0:\n",
    "            probabilities = distances / distances_sum\n",
    "        else:\n",
    "            # If all points are already centroids, choose randomly\n",
    "            probabilities = np.ones(n_samples) / n_samples\n",
    "        \n",
    "        # Choose the next centroid with probability proportional to squared distance\n",
    "        next_centroid_idx = np.random.choice(n_samples, p=probabilities)\n",
    "        centroids[k] = X[next_centroid_idx]\n",
    "    \n",
    "    return centroids\n",
    "\n",
    "# Attach this method to our KMeans class\n",
    "KMeans._kmeans_plus_plus_init = _kmeans_plus_plus_init\n",
    "\n",
    "# Create a visualization function to show the initialization difference\n",
    "def visualize_initializations(X, n_clusters=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Visualize the difference between random and K-Means++ initialization.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Dataset to visualize.\n",
    "    n_clusters : int\n",
    "        Number of clusters.\n",
    "    random_state : int\n",
    "        Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    # Initialize KMeans with random initialization\n",
    "    kmeans_random = KMeans(n_clusters=n_clusters, init='random', random_state=random_state, n_init=1)\n",
    "    random_centroids = kmeans_random._init_centroids(X)\n",
    "    \n",
    "    # Initialize KMeans with KMeans++ initialization\n",
    "    kmeans_plus_plus = KMeans(n_clusters=n_clusters, init='kmeans++', random_state=random_state, n_init=1)\n",
    "    plus_plus_centroids = kmeans_plus_plus._init_centroids(X)\n",
    "    \n",
    "    # Plot the results\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Random initialization\n",
    "    axes[0].scatter(X[:, 0], X[:, 1], c='gray', alpha=0.5)\n",
    "    axes[0].scatter(random_centroids[:, 0], random_centroids[:, 1], c='red', marker='X', s=200, label='Centroids')\n",
    "    axes[0].set_title('Random Initialization')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # KMeans++ initialization\n",
    "    axes[1].scatter(X[:, 0], X[:, 1], c='gray', alpha=0.5)\n",
    "    axes[1].scatter(plus_plus_centroids[:, 0], plus_plus_centroids[:, 1], c='red', marker='X', s=200, label='Centroids')\n",
    "    axes[1].set_title('KMeans++ Initialization')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return random_centroids, plus_plus_centroids\n",
    "\n",
    "# Visualize the two initialization methods\n",
    "random_centroids, plus_plus_centroids = visualize_initializations(X, n_clusters=5, random_state=42)\n",
    "\n",
    "print(\"Random initialization centroid distances:\")\n",
    "random_dists = []\n",
    "for i in range(len(random_centroids)):\n",
    "    for j in range(i+1, len(random_centroids)):\n",
    "        dist = np.linalg.norm(random_centroids[i] - random_centroids[j])\n",
    "        random_dists.append(dist)\n",
    "        print(f\"Distance between centroids {i} and {j}: {dist:.2f}\")\n",
    "print(f\"Average distance: {np.mean(random_dists):.2f}\")\n",
    "\n",
    "print(\"\\nK-Means++ initialization centroid distances:\")\n",
    "plus_plus_dists = []\n",
    "for i in range(len(plus_plus_centroids)):\n",
    "    for j in range(i+1, len(plus_plus_centroids)):\n",
    "        dist = np.linalg.norm(plus_plus_centroids[i] - plus_plus_centroids[j])\n",
    "        plus_plus_dists.append(dist)\n",
    "        print(f\"Distance between centroids {i} and {j}: {dist:.2f}\")\n",
    "print(f\"Average distance: {np.mean(plus_plus_dists):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1976898",
   "metadata": {},
   "source": [
    "## 5. Iterative Clustering with Convergence Checks\n",
    "\n",
    "Now let's run our K-Means algorithm on the complex dataset and visualize how it progresses through iterations until convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21448b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_kmeans_iterations(X, n_clusters=5, init='kmeans++', max_iter=10, random_state=42):\n",
    "    \"\"\"\n",
    "    Visualize the progression of K-Means algorithm through iterations.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Dataset to cluster.\n",
    "    n_clusters : int\n",
    "        Number of clusters.\n",
    "    init : str\n",
    "        Initialization method ('random' or 'kmeans++').\n",
    "    max_iter : int\n",
    "        Maximum number of iterations to visualize.\n",
    "    random_state : int\n",
    "        Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    # Create KMeans instance\n",
    "    kmeans = KMeans(n_clusters=n_clusters, init=init, max_iter=1, n_init=1, random_state=random_state)\n",
    "    \n",
    "    # Initialize centroids\n",
    "    centroids = kmeans._init_centroids(X)\n",
    "    \n",
    "    # Create a figure to visualize iterations\n",
    "    n_rows = (max_iter + 2) // 3  # +2 for initial and final states\n",
    "    fig, axes = plt.subplots(n_rows, 3, figsize=(18, 6*n_rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot initial state\n",
    "    axes[0].scatter(X[:, 0], X[:, 1], c='gray', alpha=0.5)\n",
    "    axes[0].scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200)\n",
    "    axes[0].set_title('Initial Centroids')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Variables to track convergence\n",
    "    previous_inertia = float('inf')\n",
    "    labels = None\n",
    "    \n",
    "    # Run iterations\n",
    "    for iteration in range(max_iter):\n",
    "        # Assign clusters\n",
    "        labels, min_distances = kmeans._assign_clusters(X, centroids)\n",
    "        \n",
    "        # Update centroids\n",
    "        new_centroids, empty_clusters = kmeans._update_centroids(X, labels)\n",
    "        \n",
    "        # Handle empty clusters if any\n",
    "        if empty_clusters:\n",
    "            new_centroids, labels = kmeans._handle_empty_clusters(X, labels, empty_clusters)\n",
    "        \n",
    "        # Calculate inertia\n",
    "        inertia = kmeans._calculate_inertia(X, labels, centroids)\n",
    "        \n",
    "        # Plot current state\n",
    "        axes[iteration + 1].scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "        axes[iteration + 1].scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200)\n",
    "        for i, center in enumerate(centroids):\n",
    "            axes[iteration + 1].annotate(f\"{i}\", (center[0], center[1]), \n",
    "                                 fontsize=12, ha='center', va='center',\n",
    "                                 bbox=dict(boxstyle='circle', fc='white', alpha=0.8))\n",
    "        \n",
    "        # Plot arrows showing centroid movement\n",
    "        for i in range(n_clusters):\n",
    "            axes[iteration + 1].arrow(centroids[i, 0], centroids[i, 1],\n",
    "                              new_centroids[i, 0] - centroids[i, 0],\n",
    "                              new_centroids[i, 1] - centroids[i, 1],\n",
    "                              head_width=0.5, head_length=0.7, fc='black', ec='black')\n",
    "        \n",
    "        # Check for convergence and show on plot\n",
    "        inertia_change = previous_inertia - inertia\n",
    "        rel_change = abs(inertia_change) / previous_inertia if previous_inertia > 0 else 0\n",
    "        converged = rel_change < kmeans.tol\n",
    "        \n",
    "        axes[iteration + 1].set_title(f'Iteration {iteration + 1}\\nInertia: {inertia:.2f}, Change: {rel_change:.6f}')\n",
    "        if converged:\n",
    "            axes[iteration + 1].set_facecolor('#e6ffe6')  # Light green background for convergence\n",
    "        axes[iteration + 1].grid(alpha=0.3)\n",
    "        \n",
    "        # Update for next iteration\n",
    "        previous_inertia = inertia\n",
    "        centroids = new_centroids\n",
    "        \n",
    "        # Check for early convergence\n",
    "        if converged:\n",
    "            break\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(iteration + 2, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return centroids, labels, inertia\n",
    "\n",
    "# Run K-Means with visualization of iterations\n",
    "final_centroids, final_labels, final_inertia = visualize_kmeans_iterations(\n",
    "    X, n_clusters=5, init='kmeans++', max_iter=10, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Final inertia: {final_inertia:.2f}\")\n",
    "print(f\"Number of points in each cluster: {np.bincount(final_labels)}\")\n",
    "\n",
    "# Create a function to analyze convergence speed for different initializations\n",
    "def analyze_convergence(X, n_clusters=5, max_iter=20, n_runs=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Compare convergence speed between random and K-Means++ initialization.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Dataset to cluster.\n",
    "    n_clusters : int\n",
    "        Number of clusters.\n",
    "    max_iter : int\n",
    "        Maximum number of iterations to run.\n",
    "    n_runs : int\n",
    "        Number of runs to average over.\n",
    "    random_state : int\n",
    "        Base random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    # Track inertia progress for each method\n",
    "    random_inertias = np.zeros((n_runs, max_iter))\n",
    "    kmeans_pp_inertias = np.zeros((n_runs, max_iter))\n",
    "    \n",
    "    # Run multiple times to average results\n",
    "    for run in range(n_runs):\n",
    "        # Set different seed for each run\n",
    "        run_seed = random_state + run\n",
    "        \n",
    "        # Random initialization\n",
    "        kmeans_random = KMeans(n_clusters=n_clusters, init='random', max_iter=max_iter, \n",
    "                               n_init=1, random_state=run_seed)\n",
    "        centroids = kmeans_random._init_centroids(X)\n",
    "        \n",
    "        previous_inertia = float('inf')\n",
    "        labels = None\n",
    "        \n",
    "        for iteration in range(max_iter):\n",
    "            # Assign clusters\n",
    "            labels, _ = kmeans_random._assign_clusters(X, centroids)\n",
    "            \n",
    "            # Update centroids\n",
    "            new_centroids, empty_clusters = kmeans_random._update_centroids(X, labels)\n",
    "            \n",
    "            # Handle empty clusters if any\n",
    "            if empty_clusters:\n",
    "                new_centroids, labels = kmeans_random._handle_empty_clusters(X, labels, empty_clusters)\n",
    "            \n",
    "            # Calculate inertia\n",
    "            inertia = kmeans_random._calculate_inertia(X, labels, centroids)\n",
    "            random_inertias[run, iteration] = inertia\n",
    "            \n",
    "            # Update for next iteration\n",
    "            centroids = new_centroids\n",
    "        \n",
    "        # K-Means++ initialization\n",
    "        kmeans_pp = KMeans(n_clusters=n_clusters, init='kmeans++', max_iter=max_iter, \n",
    "                           n_init=1, random_state=run_seed)\n",
    "        centroids = kmeans_pp._init_centroids(X)\n",
    "        \n",
    "        previous_inertia = float('inf')\n",
    "        labels = None\n",
    "        \n",
    "        for iteration in range(max_iter):\n",
    "            # Assign clusters\n",
    "            labels, _ = kmeans_pp._assign_clusters(X, centroids)\n",
    "            \n",
    "            # Update centroids\n",
    "            new_centroids, empty_clusters = kmeans_pp._update_centroids(X, labels)\n",
    "            \n",
    "            # Handle empty clusters if any\n",
    "            if empty_clusters:\n",
    "                new_centroids, labels = kmeans_pp._handle_empty_clusters(X, labels, empty_clusters)\n",
    "            \n",
    "            # Calculate inertia\n",
    "            inertia = kmeans_pp._calculate_inertia(X, labels, centroids)\n",
    "            kmeans_pp_inertias[run, iteration] = inertia\n",
    "            \n",
    "            # Update for next iteration\n",
    "            centroids = new_centroids\n",
    "    \n",
    "    # Calculate average inertia across runs\n",
    "    avg_random_inertias = np.mean(random_inertias, axis=0)\n",
    "    avg_kmeans_pp_inertias = np.mean(kmeans_pp_inertias, axis=0)\n",
    "    \n",
    "    # Plot convergence comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(range(1, max_iter + 1), avg_random_inertias, 'o-', label='Random Init')\n",
    "    plt.plot(range(1, max_iter + 1), avg_kmeans_pp_inertias, 's-', label='K-Means++ Init')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.title('Convergence Speed: Random vs. K-Means++ Initialization')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.yscale('log')  # Log scale to better see differences\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate number of iterations to converge (when inertia change < 1%)\n",
    "    random_converge_iter = max_iter\n",
    "    kmeans_pp_converge_iter = max_iter\n",
    "    \n",
    "    for i in range(1, max_iter):\n",
    "        # Check random init convergence\n",
    "        random_change = (avg_random_inertias[i-1] - avg_random_inertias[i]) / avg_random_inertias[i-1]\n",
    "        if random_change < 0.01 and random_converge_iter == max_iter:\n",
    "            random_converge_iter = i\n",
    "        \n",
    "        # Check k-means++ init convergence\n",
    "        kmeans_pp_change = (avg_kmeans_pp_inertias[i-1] - avg_kmeans_pp_inertias[i]) / avg_kmeans_pp_inertias[i-1]\n",
    "        if kmeans_pp_change < 0.01 and kmeans_pp_converge_iter == max_iter:\n",
    "            kmeans_pp_converge_iter = i\n",
    "    \n",
    "    print(f\"Average iterations to converge (1% threshold):\")\n",
    "    print(f\"Random Initialization: {random_converge_iter}\")\n",
    "    print(f\"K-Means++ Initialization: {kmeans_pp_converge_iter}\")\n",
    "    print(f\"Final inertia (average):\")\n",
    "    print(f\"Random Initialization: {avg_random_inertias[-1]:.2f}\")\n",
    "    print(f\"K-Means++ Initialization: {avg_kmeans_pp_inertias[-1]:.2f}\")\n",
    "\n",
    "# Analyze convergence for different initialization methods\n",
    "analyze_convergence(X, n_clusters=5, max_iter=20, n_runs=5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f51dc5",
   "metadata": {},
   "source": [
    "## 6. Handle Empty Clusters and Edge Cases\n",
    "\n",
    "K-Means can encounter several edge cases in complex datasets:\n",
    "1. Empty clusters - when no points are assigned to a centroid\n",
    "2. Outliers distorting clusters\n",
    "3. Duplicate centroids\n",
    "4. Varying cluster densities and sizes\n",
    "\n",
    "Let's demonstrate how our implementation handles these cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c223f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset that's likely to produce empty clusters\n",
    "def create_challenging_dataset(n_samples=1000, n_clusters=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Create a dataset that's likely to produce empty clusters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of samples in the dataset.\n",
    "    n_clusters : int\n",
    "        Number of target clusters.\n",
    "    random_state : int\n",
    "        Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X : numpy.ndarray\n",
    "        The generated dataset.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Create a few dense clusters\n",
    "    n_dense_clusters = n_clusters - 2\n",
    "    X = []\n",
    "    \n",
    "    # Create dense clusters close together\n",
    "    centers = []\n",
    "    for i in range(n_dense_clusters):\n",
    "        center = np.random.uniform(-5, 5, size=2)\n",
    "        centers.append(center)\n",
    "        # Create points around this center\n",
    "        cluster_size = int(n_samples * 0.8 / n_dense_clusters)\n",
    "        cluster_points = np.random.normal(loc=center, scale=0.5, size=(cluster_size, 2))\n",
    "        X.append(cluster_points)\n",
    "    \n",
    "    # Create a few distant points\n",
    "    n_distant = int(n_samples * 0.2)\n",
    "    distant_points = np.random.uniform(-20, 20, size=(n_distant, 2))\n",
    "    X.append(distant_points)\n",
    "    \n",
    "    # Combine all points and shuffle\n",
    "    X = np.vstack(X)\n",
    "    np.random.shuffle(X)\n",
    "    \n",
    "    return X\n",
    "\n",
    "# Create the challenging dataset\n",
    "challenging_X = create_challenging_dataset(n_samples=1000, n_clusters=5, random_state=42)\n",
    "\n",
    "# Visualize this dataset\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(challenging_X[:, 0], challenging_X[:, 1], c='gray', alpha=0.7)\n",
    "plt.title('Challenging Dataset for K-Means')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Define a function to demonstrate empty cluster handling\n",
    "def demonstrate_empty_cluster_handling(X, n_clusters=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Demonstrate how our K-Means implementation handles empty clusters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Dataset to cluster.\n",
    "    n_clusters : int\n",
    "        Number of clusters.\n",
    "    random_state : int\n",
    "        Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    kmeans : KMeans\n",
    "        The fitted KMeans object.\n",
    "    \"\"\"\n",
    "    # Create a standard K-Means instance with random initialization\n",
    "    # (more likely to produce empty clusters)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, init='random', max_iter=20, \n",
    "                    n_init=1, random_state=random_state)\n",
    "    \n",
    "    # Initialize the centroids in a way that's likely to create an empty cluster\n",
    "    # Place one centroid far from all data points\n",
    "    centroids = kmeans._init_centroids(X)\n",
    "    centroids[0] = np.array([30, 30])  # Place one centroid far away\n",
    "    \n",
    "    # First iteration: Create visualization of initial state\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    # Plot 1: Initial centroids\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c='gray', alpha=0.5)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200)\n",
    "    plt.title('Initial Centroids (one far away)')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Assign clusters\n",
    "    labels, _ = kmeans._assign_clusters(X, centroids)\n",
    "    \n",
    "    # Plot 2: Cluster assignments\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200)\n",
    "    \n",
    "    # Check for empty clusters\n",
    "    empty_clusters = []\n",
    "    for k in range(n_clusters):\n",
    "        if np.sum(labels == k) == 0:\n",
    "            empty_clusters.append(k)\n",
    "    \n",
    "    if empty_clusters:\n",
    "        plt.title(f'Cluster Assignments\\nEmpty clusters: {empty_clusters}')\n",
    "    else:\n",
    "        plt.title('Cluster Assignments\\nNo empty clusters')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Update centroids\n",
    "    new_centroids, empty_clusters = kmeans._update_centroids(X, labels)\n",
    "    \n",
    "    # Handle empty clusters\n",
    "    if empty_clusters:\n",
    "        new_centroids, new_labels = kmeans._handle_empty_clusters(X, labels, empty_clusters)\n",
    "    else:\n",
    "        new_labels = labels\n",
    "    \n",
    "    # Plot 3: After handling empty clusters\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=new_labels, cmap='viridis', alpha=0.7)\n",
    "    plt.scatter(new_centroids[:, 0], new_centroids[:, 1], c='red', marker='X', s=200)\n",
    "    plt.title('After Handling Empty Clusters')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Run the full algorithm from this initial state\n",
    "    centroids = new_centroids\n",
    "    labels = new_labels\n",
    "    \n",
    "    # Main loop for remaining iterations\n",
    "    for iteration in range(1, kmeans.max_iter):\n",
    "        # Assign clusters\n",
    "        labels, _ = kmeans._assign_clusters(X, centroids)\n",
    "        \n",
    "        # Update centroids\n",
    "        new_centroids, empty_clusters = kmeans._update_centroids(X, labels)\n",
    "        \n",
    "        # Handle empty clusters\n",
    "        if empty_clusters:\n",
    "            new_centroids, labels = kmeans._handle_empty_clusters(X, labels, empty_clusters)\n",
    "        \n",
    "        # Check for convergence (simplified)\n",
    "        if np.allclose(centroids, new_centroids, atol=1e-4):\n",
    "            break\n",
    "            \n",
    "        centroids = new_centroids\n",
    "    \n",
    "    # Final state visualization\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200)\n",
    "    plt.title(f'Final Clustering after {iteration+1} Iterations')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Final number of points in each cluster: {np.bincount(labels)}\")\n",
    "    \n",
    "    return kmeans\n",
    "\n",
    "# Demonstrate how our algorithm handles empty clusters\n",
    "_ = demonstrate_empty_cluster_handling(challenging_X, n_clusters=5, random_state=42)\n",
    "\n",
    "# Function to handle outliers in K-Means\n",
    "def demonstrate_outlier_handling(X, n_clusters=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Demonstrate the effect of outliers on K-Means and how to mitigate it.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Dataset to cluster.\n",
    "    n_clusters : int\n",
    "        Number of clusters.\n",
    "    random_state : int\n",
    "        Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    # Add some extreme outliers to the dataset\n",
    "    X_with_outliers = X.copy()\n",
    "    n_outliers = 10\n",
    "    outliers = np.random.uniform(-50, 50, size=(n_outliers, 2))\n",
    "    X_with_outliers = np.vstack([X_with_outliers, outliers])\n",
    "    \n",
    "    # Create standard K-Means\n",
    "    kmeans_standard = KMeans(n_clusters=n_clusters, init='kmeans++', \n",
    "                             n_init=1, random_state=random_state)\n",
    "    kmeans_standard.fit(X_with_outliers)\n",
    "    \n",
    "    # Create a visualization\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    # Plot 1: Original Data with Outliers\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.scatter(X_with_outliers[:, 0], X_with_outliers[:, 1], c='gray', alpha=0.7)\n",
    "    plt.title('Dataset with Outliers')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Standard K-Means Result\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.scatter(X_with_outliers[:, 0], X_with_outliers[:, 1], \n",
    "                c=kmeans_standard.labels_, cmap='viridis', alpha=0.7)\n",
    "    plt.scatter(kmeans_standard.centroids[:, 0], kmeans_standard.centroids[:, 1], \n",
    "                c='red', marker='X', s=200)\n",
    "    plt.title('Standard K-Means\\n(affected by outliers)')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Approach to handle outliers: Pre-filtering\n",
    "    # Calculate distance to nearest neighbor\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    nn = NearestNeighbors(n_neighbors=2)\n",
    "    nn.fit(X_with_outliers)\n",
    "    distances, _ = nn.kneighbors(X_with_outliers)\n",
    "    \n",
    "    # Use the distance to second nearest neighbor\n",
    "    outlier_scores = distances[:, 1]\n",
    "    \n",
    "    # Define outliers as points with nearest neighbor distance > 3 std dev\n",
    "    threshold = np.mean(outlier_scores) + 3 * np.std(outlier_scores)\n",
    "    outlier_mask = outlier_scores > threshold\n",
    "    \n",
    "    # Filter out outliers\n",
    "    X_filtered = X_with_outliers[~outlier_mask]\n",
    "    \n",
    "    # Run K-Means on filtered dataset\n",
    "    kmeans_filtered = KMeans(n_clusters=n_clusters, init='kmeans++', \n",
    "                             n_init=1, random_state=random_state)\n",
    "    kmeans_filtered.fit(X_filtered)\n",
    "    \n",
    "    # Predict cluster for all points including outliers\n",
    "    outlier_labels = kmeans_filtered.predict(X_with_outliers)\n",
    "    \n",
    "    # Plot 3: K-Means with Outlier Handling\n",
    "    plt.subplot(1, 3, 3)\n",
    "    # Regular points\n",
    "    plt.scatter(X_with_outliers[~outlier_mask, 0], X_with_outliers[~outlier_mask, 1], \n",
    "                c=outlier_labels[~outlier_mask], cmap='viridis', alpha=0.7)\n",
    "    # Outliers\n",
    "    plt.scatter(X_with_outliers[outlier_mask, 0], X_with_outliers[outlier_mask, 1], \n",
    "                c='red', marker='x', s=100, alpha=0.7)\n",
    "    # Centroids\n",
    "    plt.scatter(kmeans_filtered.centroids[:, 0], kmeans_filtered.centroids[:, 1], \n",
    "                c='black', marker='X', s=200)\n",
    "    \n",
    "    plt.title('K-Means with Outlier Handling\\n(Outliers marked as X)')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Number of detected outliers: {np.sum(outlier_mask)}\")\n",
    "    print(f\"True number of added outliers: {n_outliers}\")\n",
    "\n",
    "# Demonstrate outlier handling\n",
    "demonstrate_outlier_handling(challenging_X, n_clusters=5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f957d08",
   "metadata": {},
   "source": [
    "## 7. Visualize Clustering Results\n",
    "\n",
    "Let's create more advanced visualizations of our clustering results, including:\n",
    "1. Decision boundaries between clusters\n",
    "2. 3D visualization for higher dimensions\n",
    "3. Cluster size and density information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c7b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply KMeans to our original dataset\n",
    "kmeans = KMeans(n_clusters=5, init='kmeans++', n_init=10, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Create a function to visualize decision boundaries\n",
    "def plot_decision_boundaries(X, kmeans, boundary_step=0.05):\n",
    "    \"\"\"\n",
    "    Visualize decision boundaries for K-Means clustering.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        The dataset that was clustered.\n",
    "    kmeans : KMeans\n",
    "        Fitted KMeans instance.\n",
    "    boundary_step : float\n",
    "        Step size for the meshgrid.\n",
    "    \"\"\"\n",
    "    # Create a meshgrid\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, boundary_step),\n",
    "                         np.arange(y_min, y_max, boundary_step))\n",
    "    \n",
    "    # Predict cluster for each point in the meshgrid\n",
    "    Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot the decision boundary\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "    \n",
    "    # Plot the original data points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis', edgecolor='k', alpha=0.7)\n",
    "    \n",
    "    # Plot the centroids\n",
    "    plt.scatter(kmeans.centroids[:, 0], kmeans.centroids[:, 1], \n",
    "                c='red', marker='X', s=200, edgecolor='k')\n",
    "    \n",
    "    for i, centroid in enumerate(kmeans.centroids):\n",
    "        plt.annotate(f\"Cluster {i}\", (centroid[0], centroid[1]), \n",
    "                     fontsize=12, ha='center', va='center',\n",
    "                     bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.7))\n",
    "    \n",
    "    plt.title('K-Means Clustering with Decision Boundaries')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize decision boundaries\n",
    "plot_decision_boundaries(X, kmeans, boundary_step=0.1)\n",
    "\n",
    "# Create a 3D visualization (we need to generate 3D data first)\n",
    "def generate_3d_data(n_samples=1000, n_clusters=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate 3D data for visualization.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of samples.\n",
    "    n_clusters : int\n",
    "        Number of clusters.\n",
    "    random_state : int\n",
    "        Random seed.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X : numpy.ndarray\n",
    "        3D dataset.\n",
    "    true_labels : numpy.ndarray\n",
    "        True cluster labels.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Generate cluster centers\n",
    "    centers = np.random.uniform(-10, 10, size=(n_clusters, 3))\n",
    "    \n",
    "    # Initialize arrays\n",
    "    X = np.zeros((n_samples, 3))\n",
    "    true_labels = np.zeros(n_samples, dtype=int)\n",
    "    \n",
    "    # Generate points around centers\n",
    "    samples_per_cluster = n_samples // n_clusters\n",
    "    for i in range(n_clusters):\n",
    "        start_idx = i * samples_per_cluster\n",
    "        end_idx = (i + 1) * samples_per_cluster if i < n_clusters - 1 else n_samples\n",
    "        \n",
    "        # Random spread for this cluster\n",
    "        spread = np.random.uniform(0.5, 2.0)\n",
    "        \n",
    "        # Generate points\n",
    "        X[start_idx:end_idx] = np.random.normal(centers[i], spread, size=(end_idx - start_idx, 3))\n",
    "        true_labels[start_idx:end_idx] = i\n",
    "    \n",
    "    # Shuffle the data\n",
    "    idx = np.random.permutation(n_samples)\n",
    "    X = X[idx]\n",
    "    true_labels = true_labels[idx]\n",
    "    \n",
    "    return X, true_labels\n",
    "\n",
    "# Generate 3D data\n",
    "X_3d, true_labels_3d = generate_3d_data(n_samples=1000, n_clusters=5, random_state=42)\n",
    "\n",
    "# Apply KMeans to 3D data\n",
    "kmeans_3d = KMeans(n_clusters=5, init='kmeans++', n_init=10, random_state=42)\n",
    "kmeans_3d.fit(X_3d)\n",
    "\n",
    "# Create 3D visualization\n",
    "def plot_3d_clusters(X, labels, centroids, title=\"3D K-Means Clustering\"):\n",
    "    \"\"\"\n",
    "    Create a 3D visualization of clustering results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        3D dataset.\n",
    "    labels : numpy.ndarray\n",
    "        Cluster assignments.\n",
    "    centroids : numpy.ndarray\n",
    "        Centroids of clusters.\n",
    "    title : str\n",
    "        Plot title.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot data points\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], X[:, 2], \n",
    "                         c=labels, cmap='viridis', alpha=0.7, s=30)\n",
    "    \n",
    "    # Plot centroids\n",
    "    ax.scatter(centroids[:, 0], centroids[:, 1], centroids[:, 2],\n",
    "               c='red', marker='X', s=200, edgecolor='k')\n",
    "    \n",
    "    # Add cluster annotations\n",
    "    for i, centroid in enumerate(centroids):\n",
    "        ax.text(centroid[0], centroid[1], centroid[2], f\"Cluster {i}\",\n",
    "                fontsize=12, ha='center', va='center')\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.set_zlabel('Feature 3')\n",
    "    \n",
    "    # Add a color bar\n",
    "    cbar = fig.colorbar(scatter, ax=ax, pad=0.1)\n",
    "    cbar.set_label('Cluster')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize 3D clusters\n",
    "plot_3d_clusters(X_3d, kmeans_3d.labels_, kmeans_3d.centroids)\n",
    "\n",
    "# Create a function to visualize cluster characteristics\n",
    "def visualize_cluster_characteristics(X, kmeans):\n",
    "    \"\"\"\n",
    "    Visualize characteristics of clusters including size, density, and spread.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Dataset that was clustered.\n",
    "    kmeans : KMeans\n",
    "        Fitted KMeans instance.\n",
    "    \"\"\"\n",
    "    # Calculate cluster sizes\n",
    "    cluster_sizes = np.bincount(kmeans.labels_)\n",
    "    \n",
    "    # Calculate cluster densities (average distance to centroid)\n",
    "    densities = []\n",
    "    spreads = []\n",
    "    \n",
    "    for i in range(kmeans.n_clusters):\n",
    "        # Get points in this cluster\n",
    "        cluster_points = X[kmeans.labels_ == i]\n",
    "        \n",
    "        if len(cluster_points) > 0:\n",
    "            # Calculate distances to centroid\n",
    "            centroid = kmeans.centroids[i]\n",
    "            distances = np.sqrt(np.sum((cluster_points - centroid)**2, axis=1))\n",
    "            \n",
    "            # Average distance = density (lower means more dense)\n",
    "            densities.append(np.mean(distances))\n",
    "            \n",
    "            # Standard deviation of distances = spread\n",
    "            spreads.append(np.std(distances))\n",
    "        else:\n",
    "            densities.append(0)\n",
    "            spreads.append(0)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Plot cluster sizes\n",
    "    axes[0].bar(range(kmeans.n_clusters), cluster_sizes, color='skyblue')\n",
    "    axes[0].set_title('Cluster Sizes')\n",
    "    axes[0].set_xlabel('Cluster')\n",
    "    axes[0].set_ylabel('Number of Points')\n",
    "    axes[0].set_xticks(range(kmeans.n_clusters))\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot cluster densities\n",
    "    axes[1].bar(range(kmeans.n_clusters), densities, color='lightgreen')\n",
    "    axes[1].set_title('Cluster Densities\\n(Average Distance to Centroid)')\n",
    "    axes[1].set_xlabel('Cluster')\n",
    "    axes[1].set_ylabel('Average Distance')\n",
    "    axes[1].set_xticks(range(kmeans.n_clusters))\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot cluster spreads\n",
    "    axes[2].bar(range(kmeans.n_clusters), spreads, color='salmon')\n",
    "    axes[2].set_title('Cluster Spreads\\n(Standard Deviation of Distances)')\n",
    "    axes[2].set_xlabel('Cluster')\n",
    "    axes[2].set_ylabel('Standard Deviation')\n",
    "    axes[2].set_xticks(range(kmeans.n_clusters))\n",
    "    axes[2].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"Cluster Statistics:\")\n",
    "    for i in range(kmeans.n_clusters):\n",
    "        print(f\"Cluster {i}:\")\n",
    "        print(f\"  Size: {cluster_sizes[i]} points\")\n",
    "        print(f\"  Density (avg distance to centroid): {densities[i]:.4f}\")\n",
    "        print(f\"  Spread (std dev of distances): {spreads[i]:.4f}\")\n",
    "        print()\n",
    "\n",
    "# Visualize cluster characteristics\n",
    "visualize_cluster_characteristics(X, kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f89c2a",
   "metadata": {},
   "source": [
    "## 8. Evaluate Clustering Performance with Metrics\n",
    "\n",
    "To evaluate the performance of K-Means clustering, we can use several metrics:\n",
    "\n",
    "1. **Inertia**: Sum of squared distances to centroids (lower is better, but decreases with more clusters)\n",
    "2. **Silhouette Score**: Measures how similar points are to their own cluster vs other clusters (-1 to 1, higher is better)\n",
    "3. **Calinski-Harabasz Index**: Ratio of between-cluster to within-cluster dispersion (higher is better)\n",
    "4. **Davies-Bouldin Index**: Average similarity of each cluster with its most similar cluster (lower is better)\n",
    "5. **Adjusted Rand Index**: Measures similarity between two clusterings (useful when true labels are known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98794c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional evaluation metrics\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "# Define a function to compute and visualize the Elbow Method\n",
    "def plot_elbow_method(X, max_k=10, random_state=42):\n",
    "    \"\"\"\n",
    "    Plot the Elbow Method to find the optimal number of clusters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Dataset to cluster.\n",
    "    max_k : int\n",
    "        Maximum number of clusters to try.\n",
    "    random_state : int\n",
    "        Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    inertias = []\n",
    "    \n",
    "    for k in range(1, max_k + 1):\n",
    "        if k == 1:\n",
    "            # For k=1, inertia is just the sum of squared distances to the mean\n",
    "            centroid = np.mean(X, axis=0)\n",
    "            inertia = np.sum(np.sum((X - centroid)**2, axis=1))\n",
    "            inertias.append(inertia)\n",
    "        else:\n",
    "            # For k>1, use our KMeans implementation\n",
    "            kmeans = KMeans(n_clusters=k, init='kmeans++', n_init=10, random_state=random_state)\n",
    "            kmeans.fit(X)\n",
    "            inertias.append(kmeans.inertia)\n",
    "    \n",
    "    # Plot the elbow curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, max_k + 1), inertias, 'o-', color='blue')\n",
    "    plt.xlabel('Number of Clusters (k)')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.title('Elbow Method for Optimal k')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Add annotations for the \"elbow\" point\n",
    "    # Calculate the angle between consecutive points\n",
    "    angles = []\n",
    "    for i in range(1, len(inertias) - 1):\n",
    "        x1, y1 = i, inertias[i]\n",
    "        x0, y0 = i - 1, inertias[i - 1]\n",
    "        x2, y2 = i + 1, inertias[i + 1]\n",
    "        \n",
    "        # Calculate vectors\n",
    "        v1 = np.array([x1 - x0, y1 - y0])\n",
    "        v2 = np.array([x2 - x1, y2 - y1])\n",
    "        \n",
    "        # Normalize vectors\n",
    "        v1 = v1 / np.linalg.norm(v1)\n",
    "        v2 = v2 / np.linalg.norm(v2)\n",
    "        \n",
    "        # Calculate angle using dot product\n",
    "        angle = np.arccos(np.clip(np.dot(v1, v2), -1.0, 1.0))\n",
    "        angles.append(angle)\n",
    "    \n",
    "    # Potential elbow point is where angle is largest\n",
    "    elbow_idx = np.argmax(angles) + 1  # +1 because we start from the second point\n",
    "    elbow_k = elbow_idx + 1  # +1 because k starts from 1\n",
    "    \n",
    "    plt.annotate(f'Potential elbow at k={elbow_k}',\n",
    "                 xy=(elbow_k, inertias[elbow_idx]),\n",
    "                 xytext=(elbow_k + 1, inertias[elbow_idx] * 1.2),\n",
    "                 arrowprops=dict(facecolor='black', shrink=0.05, width=2))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return inertias\n",
    "\n",
    "# Plot elbow method\n",
    "inertias = plot_elbow_method(X, max_k=10, random_state=42)\n",
    "\n",
    "# Define a function to compute and visualize the Silhouette Method\n",
    "def plot_silhouette_analysis(X, max_k=10, random_state=42):\n",
    "    \"\"\"\n",
    "    Perform silhouette analysis to find the optimal number of clusters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Dataset to cluster.\n",
    "    max_k : int\n",
    "        Maximum number of clusters to try.\n",
    "    random_state : int\n",
    "        Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    silhouette_scores = []\n",
    "    k_values = range(2, max_k + 1)  # Silhouette requires at least 2 clusters\n",
    "    \n",
    "    for k in k_values:\n",
    "        # Cluster the data\n",
    "        kmeans = KMeans(n_clusters=k, init='kmeans++', n_init=10, random_state=random_state)\n",
    "        kmeans.fit(X)\n",
    "        \n",
    "        # Calculate silhouette score\n",
    "        score = silhouette_score(X, kmeans.labels_)\n",
    "        silhouette_scores.append(score)\n",
    "    \n",
    "    # Plot silhouette scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(k_values, silhouette_scores, 'o-', color='green')\n",
    "    plt.xlabel('Number of Clusters (k)')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title('Silhouette Analysis for Optimal k')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Mark the maximum silhouette score\n",
    "    best_k_idx = np.argmax(silhouette_scores)\n",
    "    best_k = k_values[best_k_idx]\n",
    "    best_score = silhouette_scores[best_k_idx]\n",
    "    \n",
    "    plt.annotate(f'Best k={best_k}, Score={best_score:.4f}',\n",
    "                 xy=(best_k, best_score),\n",
    "                 xytext=(best_k + 1, best_score - 0.05),\n",
    "                 arrowprops=dict(facecolor='black', shrink=0.05, width=2))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return silhouette_scores, best_k\n",
    "\n",
    "# Plot silhouette analysis\n",
    "silhouette_scores, best_k = plot_silhouette_analysis(X, max_k=10, random_state=42)\n",
    "\n",
    "# Evaluate the optimal clustering using multiple metrics\n",
    "def evaluate_clustering(X, true_labels=None, best_k=None, random_state=42):\n",
    "    \"\"\"\n",
    "    Evaluate clustering performance using multiple metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Dataset to cluster.\n",
    "    true_labels : numpy.ndarray or None\n",
    "        True cluster labels if available.\n",
    "    best_k : int or None\n",
    "        Number of clusters to use. If None, use 5.\n",
    "    random_state : int\n",
    "        Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    if best_k is None:\n",
    "        best_k = 5\n",
    "    \n",
    "    # Fit KMeans with the optimal k\n",
    "    kmeans = KMeans(n_clusters=best_k, init='kmeans++', n_init=10, random_state=random_state)\n",
    "    kmeans.fit(X)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    metrics = {\n",
    "        'Inertia': kmeans.inertia,\n",
    "        'Silhouette Score': silhouette_score(X, kmeans.labels_),\n",
    "        'Calinski-Harabasz Index': calinski_harabasz_score(X, kmeans.labels_),\n",
    "        'Davies-Bouldin Index': davies_bouldin_score(X, kmeans.labels_)\n",
    "    }\n",
    "    \n",
    "    if true_labels is not None:\n",
    "        metrics['Adjusted Rand Index'] = adjusted_rand_score(true_labels, kmeans.labels_)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Evaluation Metrics for k={best_k}:\")\n",
    "    for name, value in metrics.items():\n",
    "        print(f\"{name}: {value:.4f}\")\n",
    "    \n",
    "    return kmeans, metrics\n",
    "\n",
    "# Evaluate clustering with the optimal k\n",
    "optimal_kmeans, metrics = evaluate_clustering(X, true_labels=true_labels, best_k=best_k, random_state=42)\n",
    "\n",
    "# Compare evaluation metrics for different k values\n",
    "def compare_metrics_for_different_k(X, true_labels=None, max_k=10, random_state=42):\n",
    "    \"\"\"\n",
    "    Compare different evaluation metrics for various k values.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Dataset to cluster.\n",
    "    true_labels : numpy.ndarray or None\n",
    "        True cluster labels if available.\n",
    "    max_k : int\n",
    "        Maximum number of clusters to try.\n",
    "    random_state : int\n",
    "        Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    k_values = range(2, max_k + 1)\n",
    "    \n",
    "    # Initialize metric arrays\n",
    "    inertias = []\n",
    "    silhouette_scores = []\n",
    "    ch_scores = []\n",
    "    db_scores = []\n",
    "    ari_scores = []\n",
    "    \n",
    "    for k in k_values:\n",
    "        # Fit KMeans\n",
    "        kmeans = KMeans(n_clusters=k, init='kmeans++', n_init=5, random_state=random_state)\n",
    "        kmeans.fit(X)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        inertias.append(kmeans.inertia)\n",
    "        silhouette_scores.append(silhouette_score(X, kmeans.labels_))\n",
    "        ch_scores.append(calinski_harabasz_score(X, kmeans.labels_))\n",
    "        db_scores.append(davies_bouldin_score(X, kmeans.labels_))\n",
    "        \n",
    "        if true_labels is not None:\n",
    "            ari_scores.append(adjusted_rand_score(true_labels, kmeans.labels_))\n",
    "    \n",
    "    # Plot the metrics\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Inertia (lower is better, but always decreases with k)\n",
    "    axes[0, 0].plot(k_values, inertias, 'o-', color='blue')\n",
    "    axes[0, 0].set_xlabel('Number of Clusters (k)')\n",
    "    axes[0, 0].set_ylabel('Inertia')\n",
    "    axes[0, 0].set_title('Inertia vs. Number of Clusters')\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # Silhouette Score (higher is better, range: -1 to 1)\n",
    "    axes[0, 1].plot(k_values, silhouette_scores, 'o-', color='green')\n",
    "    axes[0, 1].set_xlabel('Number of Clusters (k)')\n",
    "    axes[0, 1].set_ylabel('Silhouette Score')\n",
    "    axes[0, 1].set_title('Silhouette Score vs. Number of Clusters')\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "    \n",
    "    # Calinski-Harabasz Index (higher is better)\n",
    "    axes[1, 0].plot(k_values, ch_scores, 'o-', color='purple')\n",
    "    axes[1, 0].set_xlabel('Number of Clusters (k)')\n",
    "    axes[1, 0].set_ylabel('Calinski-Harabasz Index')\n",
    "    axes[1, 0].set_title('Calinski-Harabasz Index vs. Number of Clusters')\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # Davies-Bouldin Index (lower is better)\n",
    "    axes[1, 1].plot(k_values, db_scores, 'o-', color='red')\n",
    "    axes[1, 1].set_xlabel('Number of Clusters (k)')\n",
    "    axes[1, 1].set_ylabel('Davies-Bouldin Index')\n",
    "    axes[1, 1].set_title('Davies-Bouldin Index vs. Number of Clusters')\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # If true labels are available, plot ARI\n",
    "    if true_labels is not None:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(k_values, ari_scores, 'o-', color='orange')\n",
    "        plt.xlabel('Number of Clusters (k)')\n",
    "        plt.ylabel('Adjusted Rand Index')\n",
    "        plt.title('Adjusted Rand Index vs. Number of Clusters')\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Find k with highest ARI\n",
    "        best_k_ari = k_values[np.argmax(ari_scores)]\n",
    "        print(f\"Best k according to Adjusted Rand Index: {best_k_ari}\")\n",
    "    \n",
    "    # Find best k according to each metric\n",
    "    best_k_inertia = None  # No clear criterion for inertia\n",
    "    best_k_silhouette = k_values[np.argmax(silhouette_scores)]\n",
    "    best_k_ch = k_values[np.argmax(ch_scores)]\n",
    "    best_k_db = k_values[np.argmin(db_scores)]\n",
    "    \n",
    "    print(\"Best k according to different metrics:\")\n",
    "    print(f\"Silhouette Score: {best_k_silhouette}\")\n",
    "    print(f\"Calinski-Harabasz Index: {best_k_ch}\")\n",
    "    print(f\"Davies-Bouldin Index: {best_k_db}\")\n",
    "    \n",
    "    return {\n",
    "        'inertias': inertias,\n",
    "        'silhouette_scores': silhouette_scores,\n",
    "        'ch_scores': ch_scores,\n",
    "        'db_scores': db_scores,\n",
    "        'ari_scores': ari_scores if true_labels is not None else None\n",
    "    }\n",
    "\n",
    "# Compare metrics for different k values\n",
    "metric_comparison = compare_metrics_for_different_k(X, true_labels=true_labels, max_k=10, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e50bb0",
   "metadata": {},
   "source": [
    "## 9. Compare with scikit-learn KMeans Implementation\n",
    "\n",
    "Finally, let's compare our custom K-Means implementation with scikit-learn's implementation in terms of:\n",
    "1. Clustering results\n",
    "2. Performance (execution time)\n",
    "3. Centroid positions\n",
    "4. Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ac3673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.cluster import KMeans as SklearnKMeans\n",
    "\n",
    "# Define a function to compare our implementation with scikit-learn\n",
    "def compare_kmeans_implementations(X, n_clusters=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Compare our custom KMeans implementation with scikit-learn's implementation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Dataset to cluster.\n",
    "    n_clusters : int\n",
    "        Number of clusters.\n",
    "    random_state : int\n",
    "        Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    # Our implementation\n",
    "    start_time = time.time()\n",
    "    custom_kmeans = KMeans(n_clusters=n_clusters, init='kmeans++', n_init=10, random_state=random_state)\n",
    "    custom_kmeans.fit(X)\n",
    "    custom_time = time.time() - start_time\n",
    "    \n",
    "    # scikit-learn implementation\n",
    "    start_time = time.time()\n",
    "    sklearn_kmeans = SklearnKMeans(n_clusters=n_clusters, init='k-means++', n_init=10, random_state=random_state)\n",
    "    sklearn_kmeans.fit(X)\n",
    "    sklearn_time = time.time() - start_time\n",
    "    \n",
    "    # Compare clustering results\n",
    "    custom_labels = custom_kmeans.labels_\n",
    "    sklearn_labels = sklearn_kmeans.labels_\n",
    "    \n",
    "    # Calculate agreement between clusterings\n",
    "    agreement_score = adjusted_rand_score(custom_labels, sklearn_labels)\n",
    "    \n",
    "    # Print comparison\n",
    "    print(\"=== Performance Comparison ===\")\n",
    "    print(f\"Our Implementation: {custom_time:.6f} seconds\")\n",
    "    print(f\"scikit-learn: {sklearn_time:.6f} seconds\")\n",
    "    print(f\"Speed Ratio (sklearn/custom): {sklearn_time/custom_time:.2f}x\")\n",
    "    print()\n",
    "    \n",
    "    print(\"=== Clustering Agreement ===\")\n",
    "    print(f\"Adjusted Rand Index: {agreement_score:.6f}\")\n",
    "    print(f\"(1.0 would mean identical clusterings, accounting for label permutation)\")\n",
    "    print()\n",
    "    \n",
    "    print(\"=== Inertia Comparison ===\")\n",
    "    print(f\"Our Implementation: {custom_kmeans.inertia:.2f}\")\n",
    "    print(f\"scikit-learn: {sklearn_kmeans.inertia_:.2f}\")\n",
    "    print(f\"Difference: {abs(custom_kmeans.inertia - sklearn_kmeans.inertia_):.2f}\")\n",
    "    print()\n",
    "    \n",
    "    # Compare evaluation metrics\n",
    "    custom_silhouette = silhouette_score(X, custom_labels)\n",
    "    sklearn_silhouette = silhouette_score(X, sklearn_labels)\n",
    "    \n",
    "    print(\"=== Silhouette Score Comparison ===\")\n",
    "    print(f\"Our Implementation: {custom_silhouette:.6f}\")\n",
    "    print(f\"scikit-learn: {sklearn_silhouette:.6f}\")\n",
    "    print()\n",
    "    \n",
    "    # Visualize the clustering differences\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Our implementation\n",
    "    axes[0].scatter(X[:, 0], X[:, 1], c=custom_labels, cmap='viridis', alpha=0.7)\n",
    "    axes[0].scatter(custom_kmeans.centroids[:, 0], custom_kmeans.centroids[:, 1], \n",
    "                    c='red', marker='X', s=200, edgecolor='k')\n",
    "    axes[0].set_title(f'Our Implementation\\nInertia: {custom_kmeans.inertia:.2f}, Silhouette: {custom_silhouette:.4f}')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # scikit-learn implementation\n",
    "    axes[1].scatter(X[:, 0], X[:, 1], c=sklearn_labels, cmap='viridis', alpha=0.7)\n",
    "    axes[1].scatter(sklearn_kmeans.cluster_centers_[:, 0], sklearn_kmeans.cluster_centers_[:, 1], \n",
    "                    c='red', marker='X', s=200, edgecolor='k')\n",
    "    axes[1].set_title(f'scikit-learn Implementation\\nInertia: {sklearn_kmeans.inertia_:.2f}, Silhouette: {sklearn_silhouette:.4f}')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return custom_kmeans, sklearn_kmeans\n",
    "\n",
    "# Compare our implementation with scikit-learn\n",
    "custom_kmeans, sklearn_kmeans = compare_kmeans_implementations(X, n_clusters=5, random_state=42)\n",
    "\n",
    "# Perform a detailed centroid comparison\n",
    "def compare_centroids(custom_kmeans, sklearn_kmeans):\n",
    "    \"\"\"\n",
    "    Compare centroids between our implementation and scikit-learn.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    custom_kmeans : KMeans\n",
    "        Our fitted KMeans instance.\n",
    "    sklearn_kmeans : sklearn.cluster.KMeans\n",
    "        Fitted scikit-learn KMeans instance.\n",
    "    \"\"\"\n",
    "    # Get centroids\n",
    "    custom_centroids = custom_kmeans.centroids\n",
    "    sklearn_centroids = sklearn_kmeans.cluster_centers_\n",
    "    \n",
    "    # Calculate distances between each pair of centroids\n",
    "    distances = np.zeros((custom_centroids.shape[0], sklearn_centroids.shape[0]))\n",
    "    for i in range(custom_centroids.shape[0]):\n",
    "        for j in range(sklearn_centroids.shape[0]):\n",
    "            distances[i, j] = np.sqrt(np.sum((custom_centroids[i] - sklearn_centroids[j])**2))\n",
    "    \n",
    "    # Find best matching centroids\n",
    "    from scipy.optimize import linear_sum_assignment\n",
    "    row_ind, col_ind = linear_sum_assignment(distances)\n",
    "    \n",
    "    # Visualize the matching\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c='lightgray', alpha=0.3)\n",
    "    \n",
    "    # Plot centroids with connections\n",
    "    for i, j in zip(row_ind, col_ind):\n",
    "        plt.scatter(custom_centroids[i, 0], custom_centroids[i, 1], c='blue', marker='o', s=150, label='Our' if i == 0 else \"\")\n",
    "        plt.scatter(sklearn_centroids[j, 0], sklearn_centroids[j, 1], c='green', marker='s', s=150, label='sklearn' if j == 0 else \"\")\n",
    "        plt.plot([custom_centroids[i, 0], sklearn_centroids[j, 0]],\n",
    "                 [custom_centroids[i, 1], sklearn_centroids[j, 1]], 'k--', alpha=0.5)\n",
    "        \n",
    "        # Annotate the centroids\n",
    "        plt.annotate(f\"Our {i}\", (custom_centroids[i, 0], custom_centroids[i, 1]), \n",
    "                     xytext=(10, 5), textcoords='offset points')\n",
    "        plt.annotate(f\"sklearn {j}\", (sklearn_centroids[j, 0], sklearn_centroids[j, 1]), \n",
    "                     xytext=(10, -10), textcoords='offset points')\n",
    "        \n",
    "        # Print the distance\n",
    "        midpoint = ((custom_centroids[i, 0] + sklearn_centroids[j, 0])/2,\n",
    "                    (custom_centroids[i, 1] + sklearn_centroids[j, 1])/2)\n",
    "        plt.annotate(f\"{distances[i, j]:.4f}\", midpoint, \n",
    "                     xytext=(0, 0), textcoords='offset points', ha='center')\n",
    "    \n",
    "    plt.title('Centroid Comparison: Our Implementation vs. scikit-learn')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed comparison\n",
    "    print(\"=== Centroid Distance Comparison ===\")\n",
    "    total_distance = 0\n",
    "    for i, j in zip(row_ind, col_ind):\n",
    "        dist = distances[i, j]\n",
    "        total_distance += dist\n",
    "        print(f\"Our Centroid {i} <-> sklearn Centroid {j}: Distance = {dist:.6f}\")\n",
    "    \n",
    "    print(f\"\\nAverage Centroid Distance: {total_distance/len(row_ind):.6f}\")\n",
    "    \n",
    "    return distances, row_ind, col_ind\n",
    "\n",
    "# Compare centroids between implementations\n",
    "distances, row_ind, col_ind = compare_centroids(custom_kmeans, sklearn_kmeans)\n",
    "\n",
    "# Compare on a more challenging dataset\n",
    "def compare_on_challenging_dataset(n_samples=1000, n_features=10, n_clusters=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Compare implementations on a more challenging high-dimensional dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of samples.\n",
    "    n_features : int\n",
    "        Number of features (dimensions).\n",
    "    n_clusters : int\n",
    "        Number of clusters.\n",
    "    random_state : int\n",
    "        Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    # Create a high-dimensional dataset\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Generate cluster centers\n",
    "    centers = np.random.uniform(-10, 10, size=(n_clusters, n_features))\n",
    "    \n",
    "    # Generate points\n",
    "    X_high_dim = np.zeros((n_samples, n_features))\n",
    "    true_labels = np.zeros(n_samples, dtype=int)\n",
    "    \n",
    "    samples_per_cluster = n_samples // n_clusters\n",
    "    for i in range(n_clusters):\n",
    "        start_idx = i * samples_per_cluster\n",
    "        end_idx = (i + 1) * samples_per_cluster if i < n_clusters - 1 else n_samples\n",
    "        \n",
    "        # Generate points around center with varying spread\n",
    "        spread = np.random.uniform(0.5, 2.0, size=n_features)\n",
    "        X_high_dim[start_idx:end_idx] = np.random.normal(\n",
    "            centers[i], spread, size=(end_idx - start_idx, n_features))\n",
    "        true_labels[start_idx:end_idx] = i\n",
    "    \n",
    "    # Shuffle the data\n",
    "    idx = np.random.permutation(n_samples)\n",
    "    X_high_dim = X_high_dim[idx]\n",
    "    true_labels = true_labels[idx]\n",
    "    \n",
    "    print(f\"Generated {n_samples} samples with {n_features} features and {n_clusters} clusters\")\n",
    "    \n",
    "    # Compare performance\n",
    "    start_time = time.time()\n",
    "    custom_kmeans = KMeans(n_clusters=n_clusters, init='kmeans++', n_init=5, random_state=random_state)\n",
    "    custom_kmeans.fit(X_high_dim)\n",
    "    custom_time = time.time() - start_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    sklearn_kmeans = SklearnKMeans(n_clusters=n_clusters, init='k-means++', n_init=5, random_state=random_state)\n",
    "    sklearn_kmeans.fit(X_high_dim)\n",
    "    sklearn_time = time.time() - start_time\n",
    "    \n",
    "    # Print performance comparison\n",
    "    print(\"\\n=== Performance Comparison (High-Dimensional Data) ===\")\n",
    "    print(f\"Our Implementation: {custom_time:.6f} seconds\")\n",
    "    print(f\"scikit-learn: {sklearn_time:.6f} seconds\")\n",
    "    print(f\"Speed Ratio (sklearn/custom): {sklearn_time/custom_time:.2f}x\")\n",
    "    \n",
    "    # Compare clustering quality\n",
    "    custom_ari = adjusted_rand_score(true_labels, custom_kmeans.labels_)\n",
    "    sklearn_ari = adjusted_rand_score(true_labels, sklearn_kmeans.labels_)\n",
    "    \n",
    "    print(\"\\n=== Clustering Quality Comparison ===\")\n",
    "    print(f\"Our Implementation ARI: {custom_ari:.6f}\")\n",
    "    print(f\"scikit-learn ARI: {sklearn_ari:.6f}\")\n",
    "    \n",
    "    # Compare inertia\n",
    "    print(\"\\n=== Inertia Comparison ===\")\n",
    "    print(f\"Our Implementation: {custom_kmeans.inertia:.2f}\")\n",
    "    print(f\"scikit-learn: {sklearn_kmeans.inertia_:.2f}\")\n",
    "    \n",
    "    return custom_kmeans, sklearn_kmeans, X_high_dim, true_labels\n",
    "\n",
    "# Compare on a high-dimensional dataset\n",
    "high_dim_custom, high_dim_sklearn, X_high_dim, high_dim_true = compare_on_challenging_dataset(\n",
    "    n_samples=2000, n_features=10, n_clusters=7, random_state=42)\n",
    "\n",
    "# Final summary and conclusions\n",
    "print(\"\\n===== Final Comparison Summary =====\")\n",
    "print(\"1. Implementation Differences:\")\n",
    "print(\"   - Our implementation includes detailed handling of empty clusters\")\n",
    "print(\"   - Our implementation has more visualization and analysis capabilities\")\n",
    "print(\"   - scikit-learn is optimized for performance and has more options\")\n",
    "\n",
    "print(\"\\n2. Performance:\")\n",
    "print(\"   - scikit-learn is typically faster due to optimized C/C++ code\")\n",
    "print(\"   - Our implementation is more transparent and educational\")\n",
    "\n",
    "print(\"\\n3. Clustering Quality:\")\n",
    "print(\"   - Both implementations produce similar clustering results\")\n",
    "print(\"   - Differences in results are mostly due to different random initializations\")\n",
    "print(\"   - scikit-learn might converge better due to its optimized implementation\")\n",
    "\n",
    "print(\"\\n4. When to Use Each:\")\n",
    "print(\"   - Use scikit-learn for production and large datasets\")\n",
    "print(\"   - Use our implementation for learning and understanding K-Means\")\n",
    "print(\"   - Our implementation allows easier customization and visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8370d518",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "In this notebook, we've implemented a comprehensive K-Means clustering algorithm from scratch and compared it with scikit-learn's implementation. Here's what we've learned:\n",
    "\n",
    "1. **K-Means Fundamentals**:\n",
    "   - The core algorithm is simple but powerful\n",
    "   - Initialization significantly affects results (K-Means++ vs Random)\n",
    "   - Convergence typically occurs within 10-20 iterations\n",
    "\n",
    "2. **Advanced Implementation Aspects**:\n",
    "   - Handling empty clusters is crucial for robustness\n",
    "   - Dealing with outliers improves clustering quality\n",
    "   - Multiple runs with different initializations help find better solutions\n",
    "\n",
    "3. **Evaluation Techniques**:\n",
    "   - Elbow method helps determine optimal cluster count\n",
    "   - Silhouette score provides insight into cluster quality\n",
    "   - Multiple metrics should be considered for thorough evaluation\n",
    "\n",
    "4. **Real-world Considerations**:\n",
    "   - K-Means struggles with non-spherical or unequally sized clusters\n",
    "   - High-dimensional data presents additional challenges\n",
    "   - Preprocessing (scaling, outlier removal) is often essential\n",
    "\n",
    "5. **Custom vs. Library Implementation**:\n",
    "   - Our implementation offers transparency and educational value\n",
    "   - scikit-learn offers performance and robustness\n",
    "   - Both provide similar clustering results\n",
    "\n",
    "Understanding the intricacies of K-Means implementation equips data scientists with the knowledge to better apply and interpret this widely-used clustering algorithm in real-world scenarios."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
